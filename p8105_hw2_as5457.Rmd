---
title: "p8105_hw2_as5457"
author: "Sunny Siddique"
date: "October 1, 2018"
output: github_document
---

-------------------------------------------------------------------------------------------------------------------
#Problem 1


**Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or recode function may be useful).**

```{r transit_df}
#Loading the tidyverse package
library(tidyverse)

#Reading in the CSV File
transit_data = read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 

#Cleaning variable names
  janitor::clean_names(dat = .) %>%     
  
#Selecting relevant variables
    select (line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>%   
  
#Converting the entry variable from character to logical.
  mutate(entry=ifelse(entry == "YES", yes = TRUE, no = FALSE))  
class(transit_data$entry)
```

**Write a short paragraph about this dataset and explain briefly what variables the dataset contains, describe your data cleaning steps so far, and give the dimension (rows x columns) of the resulting dataset. Are these data tidy?**

The variables in this dataset provide information regarding the line, name, exact location (latitude/longitude), the name and number of routes available at the station, entry, vending and ADA compliance status. To clean this dataset, I first used the janitor package to change the variable names to lower case and variable names with multiple words were separated by "_". Then selected the variables that were specified in the prompt and used the mutate and ifthen function to change the character variable "entry" into a logical variable. The dimension of the resulting dataset is `r dim_desc(transit_data)`. This dataset is not tidy because the route variables share similar data on repeated columns. Therefore, to make this dataset more tidy we can combine the route variables into two columns indicating the route number and route name.*


**How many distinct stations are there? Note that stations are identified both by name and by line (e.g. 125th St A/B/C/D; 125st 1; 125st 4/5); the distinct function may be useful here.**

*The number of distinct stations is `r nrow(distinct(transit_data, station_name, line))`.*


**How many stations are ADA compliant?**

*84 stations are ADA compliant.*

```{r}
#Creating a new object called station_ADA from transit_data dataset
station_ADA = transit_data %>% 

#Filtering data for stations that are ADA compliant
  filter (ada == "TRUE") %>% 
  
#Choosing rows that have distinct station names and lines
  distinct (station_name,line)

#Counting the number of rows in the resulting dataset
nrow(station_ADA)
```


**What proportion of station entrances / exits without vending allow entrance?**

*The proportion of station entrances/exits without vending that allow entrance are 37.70%.*

```{r}
#Defining an object called numerator that counts the number of stations without vending that allow entrance.
numerator =
nrow(transit_data %>% 
filter (vending == "NO", entry == "TRUE"))

#Defining an object called denominator that counts the number of stations without vending.
denominator = 
  nrow(transit_data %>% 
         filter(vending == "NO"))

#Calculating the proportion of station entrances/exits without vending that allow entrance.
prop = numerator/denominator
prop
```

**Reformat data so that route number and route name are distinct variables.** 

```{r}
#renaming route variables with _ to be able to split by "_" 
transit_split = transit_data %>% 
  rename (route_1 = route1, route_2 = route2, route_3 = route3, route_4 = route4, route_5 = route5, route_6 = route6, route_7 = route7, route_8 = route8, route_9 = route9, route_10 = route10, route_11 = route11) %>% 
  
#Creating an aggregate variable that gathers the route_1 to route_11 variables into one column.
  gather(key = route, value = route_name, route_1:route_11) %>%
  
#Splitting the route variable into two strings: one counting the word route and the other containing the route number
  separate (route, into = c("route_str", "route_number"), sep = "_")
```


**How many distinct stations serve the A train?**

*The number of distinct stations that serve the A train is 60.*

```{r}
#Creating an object called a_train that keeps only stations that serve the A train and counts the number of distinct stations.
a_train = transit_split %>% 
  filter (route_name == "A") %>% 
  distinct(station_name, line)
nrow(a_train)
```

**Of the stations that serve the A train, how many are ADA compliant?**

*Of the stations that serve the A train, 17 stations are ADA compliant.*

```{r}
#Creating an object called ada_comp that keeps only stations that serve the A train and are ADA compliant and then counting the number of distinct stations that serve the A train and are ADA compliant. 
ada_comp = transit_split %>% 
  filter (route_name == "A", ada == "TRUE") %>% 
  distinct(station_name, line)
nrow(ada_comp)
```

#Problem 2

**Read and clean the Mr. Trash Wheel sheet:**

- Specify the sheet in the Excel file and to omit columns containing notes (using the range argument and cell_cols() function)
- Use reasonable variable names
- Omit rows that do not include dumpster-specific data, rounds the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r}
#Reading in the Mr. Trash Wheel dataset
library(readxl)
trash_wheel = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 1, range = "A2:N336") %>% 
  
#Cleaning variable names
  janitor::clean_names(dat = .) %>% 
  
#Filtering out rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) %>% 
  
#Rounding the number of sports balls to the nearest integer and converting the result to an integer variable.
  mutate (sports_balls = as.integer(round(sports_balls, digits = 0)))

#Viewing the resulting dataset
trash_wheel
```


**Read and clean precipitation data for 2016 and 2017. For each, omit rows without precipitation data and add a variable year.**

```{r}
#Reading in the 2017 precipitation data
prec17 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 4, range = "A2:B14") %>% 
  
#Cleaning variable names
  janitor::clean_names(dat = .) %>% 
  
#Filtering out any missing values
  filter(!is.na(total))

#Adding a variable called "year"
prec17$year = 2017
```

```{r}
#Reading in the 2017 precipitation data
prec16 = readxl::read_excel("./data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", sheet = 5, range = "A2:B14") %>%
  
#Cleaning variable names
  janitor::clean_names(dat = .) %>% 
  
#Filtering out any missing values
  filter(!is.na(total))

#Adding a variable called "year"
prec16$year = 2016
```


**Next, combine datasets and convert month to a character variable (the variable month.name is built into R and should be useful).**

```{r}
#Combining the 2016 and 2017 precipitation datasets.
combined_year = bind_rows(prec16, prec17) %>% 
  
#Converting month to a character variable.
  mutate (month = month.name[month])
```


**Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in both resulting datasets, and give examples of key variables. For available data, what was the total precipitation in 2017? What was the median number of sports balls in a dumpster in 2016?**

* The number of observations in the trashwheel dataset is `r nrow (trash_wheel)`. 
* The number of observations in the merged precipitation dataset is `r nrow (combined_year)`. 
* The key variables in the trash_wheel dataset include variables that describe the amount of trash that was collected in each dumpster (by weight and volume) and also the various types of trash that were collected (plastic bottles, grocery bags etc.). 
* Key variables in the precipitation dataset include the amount of precipitation in 2016 and 2017. 
* Total precipitation in 2017 was `r sum(prec17$total)`.
* The median number of sports balls in a dumpster in 2016 was `r median(trash_wheel$sports_balls)`.

#Problem 3

**This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the  p8105.datasets package.**

For this question:

- Format the data to use appropriate variable names;
- Focus on the “Overall Health” topic
- Exclude variables for class, topic, question, sample size, and everything from lower confidence limit to GeoLocation
- Structure data so that values for Response (“Excellent” to “Poor”) are column names / variables which indicate the proportion of subjects with each response (which are values of Data_value in the original dataset)
- Create a new variable showing the proportion of responses that were “Excellent” or “Very Good”

```{r}
#Loading the dataset from the p8105.datasets package
library(p8105.datasets)
data("brfss_smart2010")

#Defining a new dataset called brfss_smart from the original dataset
brfss_smart = brfss_smart2010 %>% 
  
#Cleaning variable names
janitor::clean_names(dat = .) %>% 
  
#Keeping only rows where the topic was "Overall Health"
  filter(topic == "Overall Health") %>% 

#Excluding the specified variables by just selecting the variables that are not to be excluded.
  select(year, locationabbr, locationdesc, response, data_value) %>% 
  
#Converting response (“Excellent” to “Poor”) are column names / variables
  spread(., key = response, value = data_value) %>% 
  
#Re-cleaning variable names
  janitor::clean_names() %>% 
  
#Creating a new variable showing the proportion of responses that were “Excellent” or “Very Good”
  mutate(prop_excellent_vgood = excellent + very_good)
  
```



**How many unique locations are included in the dataset? Is every state represented? What state is observed the most?**

*The number of unique locations is `r length(unique(brfss_smart$locationabbr))`. This includes all 50 states and Washington DC. The state that is observed the most is `r tail(names(sort(table(brfss_smart$locationabbr))), 1)`.*



**In 2002, what is the median of the “Excellent” response value?**

*The median of the excellent response value is 23.6.*

```{r}
#Defining a new object called median_excellent that pulls and filters data from brfss_smart.
median_excellent = brfss_smart %>%
  
#Omitting any missing vallues
  na.omit() %>% 
  
#Defining the aggregate function group_by function to group by year.
  group_by(year) %>%
  
#Using summarize function to calculate median of the "Excellent" response value.
  summarize(median_ex = median(excellent)) %>% 
  
#Filtering results to show only the value for 2002.
  filter (year == 2002)

#Showing the final remaining cell. 
median_excellent
```



**Make a histogram of “Excellent” response values in the year 2002.**


```{r}
#Loading the appropriate package
library(ggridges)

#Defining the histogram as excellent_hist which pulls from the brfss_smart dataset
excellent_hist = brfss_smart %>% 
  
#Filtering the dataset to year = 2002
  filter(year == 2002) %>% 
  
#Creating ggplot with x values as the proportion with excellent condition
  ggplot(., aes(x = excellent)) + 
  geom_histogram()

#Viewing the histogram
excellent_hist

#Saving histogram to project directory using ggsave.
ggsave("histogram_excellent.pdf", plot = excellent_hist)
```



**Make a scatterplot showing the proportion of “Excellent” response values in New York County and Queens County (both in NY State) in each year from 2002 to 2010.**

```{r}
#Creating a ny_county dataset that contains only data for New York county.
ny_county = brfss_smart %>% 
  filter(locationdesc == "NY - New York County")

#Creating a queens_county dataset that contains only data for Queens county. 
queens_county = brfss_smart %>%
  filter(locationdesc == "NY - Queens County")

#Combining the New York County and Queens County datasets
combined_county = bind_rows(ny_county, queens_county)

#Creating scatterplot showing the proportion of "Excellent" response values in the two counties. 
scatter_county = ggplot(combined_county, aes(x = year, y = excellent)) + 
  geom_point(aes(color = locationdesc))

#Viewing the scatterplot
scatter_county

#Saving scatterplot to project directory using ggsave.
ggsave("scatter_county.pdf", plot = scatter_county)
```

